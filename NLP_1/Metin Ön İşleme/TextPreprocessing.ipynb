{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41392ca6-79f0-4e23-91e2-ac473855601d",
   "metadata": {},
   "source": [
    "## Veri Temizleme\n",
    "**Metin verilerini analiz edilebilir hale getirmek için yapılan bir dizi işlemi içerir. Model performansını olumsuz etkileyebilecek öğeler kaldırılır veya düzeltilir.** <br> <br>\n",
    "-Boşlukların Temizlenmesi <br>\n",
    "-Büyük-Küçük Harf Dönüşümleri <br>\n",
    "-Noktalama İşaretlerinin Kaldırılması <br>\n",
    "-Özet Karakterlerin Kaldırılması <br>\n",
    "-Yazım Hatalarının Düzeltilmesi <br>\n",
    "-HTML ve URL Temizleme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1039792c-18e1-4581-b873-40aa56209bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split:  ['Hello,', 'World!', '2025'] \n",
      "\n",
      "text: Hello,      World!         2025 \n",
      "cleaned_text1: Hello, World! 2025\n"
     ]
    }
   ],
   "source": [
    "# metinlerde bulunan fazla boşlukları ortadan kaldır\n",
    "\n",
    "text = \"Hello,      World!         2025\"\n",
    "split_text = text.split()\n",
    "print(\"Split: \",split_text,\"\\n\")\n",
    "\n",
    "cleaned_text1 = \" \".join(text.split())\n",
    "print(f\"text: {text} \\ncleaned_text1: {cleaned_text1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a12f1982-7086-4f6a-a690-df5f37dcf2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: HeLLo, wOrld! 2025 \n",
      "cleaned_text2: hello, world! 2025\n"
     ]
    }
   ],
   "source": [
    "# büyük harf --> küçük harf çevrimi\n",
    "\n",
    "text = \"HeLLo, wOrld! 2025\"\n",
    "\n",
    "cleaned_text2 = text.lower()\n",
    "print(f\"text: {text} \\ncleaned_text2: {cleaned_text2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3152345-9217-4e46-ba76-098de7e3ea15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: Hello, World! 2025. \n",
      "cleaned_text3: Hello World 2025\n"
     ]
    }
   ],
   "source": [
    "# noktalama işaretlerini kaldır\n",
    "import string\n",
    "\n",
    "text = \"Hello, World! 2025.\"\n",
    "\n",
    "cleaned_text3 = text.translate(str.maketrans(\"\",\"\", string.punctuation))\n",
    "print(f\"text: {text} \\ncleaned_text3: {cleaned_text3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0893d09c-6cde-4423-b0da-26fea4ad0d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: Hello, World! 2025 # % & \n",
      "cleaned_text4: Hello World 2025   \n"
     ]
    }
   ],
   "source": [
    "# özel karakterleri kaldır\n",
    "import re\n",
    "\n",
    "text = \"Hello, World! 2025 # % &\"\n",
    "cleaned_text4 = re.sub(r\"[^A-Za-z0-9\\s]\", \"\", text)\n",
    "print(f\"text: {text} \\ncleaned_text4: {cleaned_text4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a1f93d2-eb4f-4ccb-ad7e-787c1aaa7307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yazım hatalarını düzelt\n",
    "# !pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec9523ce-8111-4a77-b825-7f4b9ac88e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: Hellıo, Wirld! 2025 \n",
      "cleaned_text5: Hello, World! 2025\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob # metin analizlerinde kullanılan bir kütüphane\n",
    "\n",
    "text = \"Hellıo, Wirld! 2025\"\n",
    "cleaned_text5 = TextBlob(text).correct() # yazım hataların düzeltir\n",
    "print(f\"text: {text} \\ncleaned_text5: {cleaned_text5}\") # İngilizce için iyi, Türkçe’de önerilmez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c56a85a0-41ae-4a13-8d7b-2885842edef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: <div>Hello, World! 2025</div> \n",
      "cleaned_text6: Hello, World! 2025\n"
     ]
    }
   ],
   "source": [
    "# html yada url etiketlerini kaldırma\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html_text = \"<div>Hello, World! 2025</div>\"\n",
    "cleaned_text6 = BeautifulSoup(html_text, \"html.parser\").get_text()\n",
    "print(f\"text: {html_text} \\ncleaned_text6: {cleaned_text6}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adbc5b6-2706-4919-8306-826e75fe7903",
   "metadata": {},
   "source": [
    "## Tokenizasyon\n",
    "**Kısaca tokenization, bir metni daha küçük parçalara ayırmaa işlemidir. Harf, kelime veya cümle bazlı yapılabilir.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5103b247-ab96-4da2-9c72-7bfd5f2e9350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk\n",
    "# !pip install --upgrade nltk\n",
    "\n",
    "import nltk # natural language toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9428dcfb-4ed2-48f5-bb0e-27791b78e533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kelime Tokenizasyonu:  ['Hello', ',', 'World', '!', 'How', 'are', 'you', '?', 'Hello', ',', 'hi', '...']\n",
      "Cümle Tokenizasyonu: ['Hello, World!', 'How are you?', 'Hello, hi ...']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Ensar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"punkt\") # metni kelime ve cümle bazında tokenlara ayırabilmek için gerekli\n",
    "\n",
    "text = \"Hello, World! How are you? Hello, hi ...\"\n",
    "\n",
    "# ✔️ kelime tokenizasyonu: word_tokenize --> metni kelimelere ayırır.\n",
    "# noktalama işaretleri ve boşlukları ayrı birer token olarak elde edilecektir.\n",
    "word_tokens = nltk.word_tokenize(text)\n",
    "print(\"Kelime Tokenizasyonu: \",word_tokens)\n",
    "\n",
    "# ✔️ cümle tokenizasyonu: sent_tokenize --> metni cümlelere ayırır, her bir cümle birer token olur. \n",
    "sentence_tokens = nltk.sent_tokenize(text)\n",
    "print(\"Cümle Tokenizasyonu:\",sentence_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34664df4-ac3d-419e-84e0-18ae0f601991",
   "metadata": {},
   "source": [
    "## Stemming (Kök Bulma)\n",
    "**Kısaca, kelimelerin kök formunu (yani temel anlamını) bulmak için kelimenin sonundaki eklerin (suffix) çıkarılması işlemidir.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee357996-0d6c-4c63-b175-1b4fbd0f43d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer \n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "words = [\"running\", \"runner\", \"ran\", \"runs\", \"better\", \"go\", \"went\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e52264a-f64d-4c4d-a6cf-4c58db7251af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stems: ['run', 'runner', 'ran', 'run', 'better', 'go', 'went']\n"
     ]
    }
   ],
   "source": [
    "stems = [stemmer.stem(w) for w in words]\n",
    "print(f\"Stems: {stems}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46ec9ef-64ae-4d86-a93a-b7ba1c402bc6",
   "metadata": {},
   "source": [
    "## Lemmatization (Gövdeleme)\n",
    "**Kısaca, kelimenin anlamını ve dilbilgisel yapısını dikkate alarak doğru bir kök bulmaya çalışır**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c74640d-82f9-4730-b962-115e0a991463",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Ensar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet') # lemmatization işlemi için gerekli olan veri tabanı\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fcd347a8-2fe7-4a1b-b6ee-055b71cd02fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmas: ['run', 'runner', 'run', 'run', 'better', 'go', 'go']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemmas = [lemmatizer.lemmatize(w, pos=\"v\") for w in words] # v = verb\n",
    "print(f\"Lemmas: {lemmas}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff79b74f-1ba2-4bdb-8049-4a500dc9ed71",
   "metadata": {},
   "source": [
    "## Stop Words\n",
    "**Metinlerde genellikle anlamı çok az olan veya metnin analizi sırasında çok faydalı olmayan kelimelerdir.** <br>\n",
    "*Örneğin:* ve, ile, gibi | the, it, of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1053eb39-05eb-4e04-8060-0e5ffd0a2cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'him', 'until', 'yours', 'more', 's', 'how', 'but', 'are', 'y', 'you', 'shouldn', 'hadn', \"didn't\", 'with', 'weren', \"they've\", \"mightn't\", 'themselves', 'herself', 'than', 'own', 'will', 'didn', 'when', 're', 'd', 'other', 'too', 'should', 'as', \"you'd\", \"she's\", 'where', \"you're\", 'a', 'only', 'were', 'from', 'don', \"don't\", 'again', 'very', 'by', 'aren', 'needn', 'its', 'nor', 'll', 'or', 'o', \"wouldn't\", 'we', 'what', 'itself', 'here', 'some', 'why', 'i', 'before', 'this', 'm', 'same', 'them', 'theirs', 'who', 'can', 'both', 'any', 'do', 'to', \"isn't\", 'himself', 'their', 'into', \"i'd\", \"shan't\", 'ourselves', 'over', 'through', 'was', \"we've\", \"you've\", \"couldn't\", 'there', 'did', 'because', \"i've\", 'most', 'she', 'on', \"they're\", 'those', 'is', 'few', 'myself', \"should've\", 'if', 'such', \"hasn't\", \"wasn't\", 'ma', 't', 'of', 'all', 'then', 'her', \"won't\", 'doesn', 'he', 'hers', 'so', 'it', 'out', 'mustn', \"she'd\", 'above', 'does', 'these', 've', \"weren't\", 'am', 'been', 'doing', \"i'm\", 'during', 'be', 'for', 'off', 'up', 'my', 'me', 'once', 'mightn', 'whom', \"they'll\", 'the', 'shan', \"aren't\", 'against', 'in', \"we'll\", 'yourself', \"he'd\", \"she'll\", 'while', 'about', \"i'll\", 'his', 'having', \"it'd\", 'below', 'wouldn', \"haven't\", \"it's\", \"he's\", \"needn't\", 'under', 'being', 'your', 'further', 'haven', 'not', 'now', 'had', \"that'll\", 'they', 'hasn', 'wasn', 'our', \"we'd\", 'each', \"you'll\", 'between', \"he'll\", 'won', 'ours', \"it'll\", 'yourselves', 'ain', 'down', 'an', 'which', \"hadn't\", 'and', 'have', 'just', 'no', \"doesn't\", 'isn', 'couldn', 'has', \"mustn't\", 'that', 'after', 'at', \"they'd\", \"shouldn't\", \"we're\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ensar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\") # farklı dillerde en çok kullanılan stop words içeren veri seti\n",
    "\n",
    "# ingilizce stop words analizi\n",
    "stop_words_eng = set(stopwords.words(\"english\"))\n",
    "print(stop_words_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15ff8e18-ea68-4b5d-9daa-cc9fbb7e94ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kelime Tokenizasyonu:  ['There', 'are', 'some', 'examples', 'of', 'handling', 'stop', 'words', 'from', 'some', 'texts']\n",
      "Stop Word' lerden filtrelenmiş hali: ['examples', 'handling', 'stop', 'words', 'texts']\n"
     ]
    }
   ],
   "source": [
    "# örnek metin\n",
    "text = \"There are some examples of handling stop words from some texts\"\n",
    "\n",
    "# tokenization\n",
    "text_list = nltk.word_tokenize(text)\n",
    "print(\"Kelime Tokenizasyonu: \",text_list)\n",
    "\n",
    "# eğer word ingilizce stop word listesinde (stop_words_eng) yoksa, bu kelimeyi filtrelenmiş listeye ekliyoruz.\n",
    "filtered_words = [word for word in text_list if word.lower() not in stop_words_eng]\n",
    "print(f\"Stop Word' lerden filtrelenmiş hali: {filtered_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e45b08b3-6629-4c3f-a511-5989bdd1ccd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sanki', 'hiç', 'belki', 'nerede', 'niye', 'o', 'hepsi', 'biz', 'kez', 'aslında', 'az', 'ne', 'da', 'şu', 'mü', 'nasıl', 'ile', 'neden', 'her', 'biri', 'gibi', 'mı', 've', 'tüm', 'eğer', 'mu', 'yani', 'çok', 'siz', 'niçin', 'nereye', 'bazı', 'hem', 'için', 'şey', 'daha', 'defa', 'acaba', 'ya', 'diye', 'ama', 'en', 'nerde', 'birşey', 'ise', 'çünkü', 'kim', 'veya', 'de', 'hep', 'birkaç', 'bu', 'ki'}\n"
     ]
    }
   ],
   "source": [
    "# türkçe stop words analizi\n",
    "stop_words_tr = set(stopwords.words(\"turkish\"))\n",
    "print(stop_words_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2309dad6-be43-4ab1-90a0-5a0e8981f4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kelime Listesi: ['merhaba', 'arkadaşlar', 'bu', 'derste', 'çok', 'güzel', 'ilerliyoruz.']\n",
      "Stop Word'lerden filtrelenmiş hali: ['merhaba', 'arkadaşlar', 'derste', 'güzel', 'ilerliyoruz.']\n"
     ]
    }
   ],
   "source": [
    "# örnek metin\n",
    "metin = \"merhaba arkadaşlar bu derste çok güzel ilerliyoruz.\"\n",
    "\n",
    "# tokenization\n",
    "word_list = metin.split()\n",
    "print(\"Kelime Listesi:\", word_list)\n",
    "\n",
    "filter_metin = []\n",
    "for w in word_list:\n",
    "    if w.lower() not in stop_words_tr:\n",
    "        filter_metin.append(w)\n",
    "\n",
    "print(f\"Stop Word'lerden filtrelenmiş hali: {filter_metin}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec49af98-56c2-4293-bd65-815a698c2df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bu', 'bir', 'denemedir.', 'Amacımız', 'bu', 'metinde', 'bulunan', 'özel', 'karakterlerini', 'elemek', 'mi', 'acaba?']\n"
     ]
    }
   ],
   "source": [
    "# kütüphanesiz stop words çıkarımı\n",
    "my_stop_words_list = [\"için\", \"bu\", \"ile\", \"mu\", \"mi\", \"özel\"]\n",
    "\n",
    "metin = \"Bu bir denemedir. Amacımız bu metinde bulunan özel karakterlerini elemek mi acaba?\"\n",
    "\n",
    "metin_list = metin.split()\n",
    "print(metin_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d5bb359-856d-4f54-86eb-87faef5c8299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop Word' lerden arındırılmı hali: ['bir', 'denemedir.', 'Amacımız', 'metinde', 'bulunan', 'karakterlerini', 'elemek', 'acaba?']\n",
      "Metinden çıkarılan stop words' ler:  {'bu', 'mi', 'özel'}\n"
     ]
    }
   ],
   "source": [
    "metin_filter = [word for word in metin_list if word.lower() not in my_stop_words_list]\n",
    "metin_stop_words = set([word.lower() for word in metin_list if word.lower() in my_stop_words_list])\n",
    "\n",
    "print(\"Stop Word' lerden arındırılmı hali:\", metin_filter)\n",
    "print(\"Metinden çıkarılan stop words' ler: \", metin_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb443b2-394c-46fc-8951-8366670f7b12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
