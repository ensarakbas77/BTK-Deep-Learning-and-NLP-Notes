{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6472cbf-eda9-45b0-814f-ff739aad40e7",
   "metadata": {},
   "source": [
    "### Ödev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7aa2007-270d-4a7a-adb9-796fc05f0539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TextProcessing notebook' u içerisinde anlatılanları tekrar etmek amacıyla yaptığım pratikler."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff36c3c6-b79c-467b-ba1a-c245109692aa",
   "metadata": {},
   "source": [
    "### Veri Temizleme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "838a6048-f576-4cc7-8cb3-0dd823c3d6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Küçük harfe dönüştürme işleminden sonraki metin: ===\n",
      "harika bir ürün!!!  kesinlikle tavsiye ediyorum...     kargo çok hızlıydı ama paket biraz ezilmişti :(   #alışveriş @trendvagonu\n",
      "\n",
      "=== noktalama işaretlerini kaldırdıktan sonra oluşan metin: ===\n",
      "harika bir ürün  kesinlikle tavsiye ediyorum     kargo çok hızlıydı ama paket biraz ezilmişti    alışveriş trendvagonu\n"
     ]
    }
   ],
   "source": [
    "# büyük harf --> küçük harf çevrimi\n",
    "text = \"Harika bir ürün!!!  Kesinlikle tavsiye ediyorum...     Kargo çok hızlıydı ama paket biraz ezilmişti :(   #alışveriş @trendvagonu\"\n",
    "text = text.lower()\n",
    "print(f\"=== Küçük harfe dönüştürme işleminden sonraki metin: ===\\n{text}\")\n",
    "\n",
    "# noktalama işaretlerini kaldırma\n",
    "import string\n",
    "\n",
    "text = text.translate(str.maketrans(\"\",\"\", string.punctuation))\n",
    "print(f\"\\n=== noktalama işaretlerini kaldırdıktan sonra oluşan metin: ===\\n{text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00201ebb-3888-4972-813d-033d48449a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== boşlukları kaldırdıktan sonra oluşna metin: ===\n",
      "harika bir ürün kesinlikle tavsiye ediyorum kargo çok hızlıydı ama paket biraz ezilmişti alışveriş trendvagonu\n"
     ]
    }
   ],
   "source": [
    "# boşlukları temizleme\n",
    "text = \" \".join(text.split()) \n",
    "print(f\"=== boşlukları kaldırdıktan sonra oluşna metin: ===\\n{text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6166877c-37ac-462f-ac64-43534b12d688",
   "metadata": {},
   "source": [
    "### Tokenizasyon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74173af8-2ae7-413b-abf8-598454efb113",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download(\"punkt\") # metni kelime ve cümle bazında tokenlara ayırabilmek için gerekli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c9c551a-b035-48f7-8110-8d1d14599ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== kelime bazlı tokenizasyon: ===\n",
      "['harika', 'bir', 'ürün', 'kesinlikle', 'tavsiye', 'ediyorum', 'kargo', 'çok', 'hızlıydı', 'ama', 'paket', 'biraz', 'ezilmişti', 'alışveriş', 'trendvagonu']\n"
     ]
    }
   ],
   "source": [
    "word_tokens = nltk.word_tokenize(text)\n",
    "print(f\"=== kelime bazlı tokenizasyon: ===\\n{word_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a53bd800-f1f9-4372-855e-216d3f3e7cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== cümle bazlı tokenizasyon: ===\n",
      "['Bu bir ödevdir.', 'Veri ön işleme pratiği yapıyorum.', 'Öğreniyorum...']\n"
     ]
    }
   ],
   "source": [
    "metin = \"Bu bir ödevdir. Veri ön işleme pratiği yapıyorum. Öğreniyorum...\"\n",
    "sentence_tokens = nltk.sent_tokenize(metin)\n",
    "print(f\"=== cümle bazlı tokenizasyon: ===\\n{sentence_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cef87c1-07b9-40c0-b56e-982a9a5d386a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'harika bir ürün kesinlikle tavsiye ediyorum kargo çok hızlıydı ama paket biraz ezilmişti alışveriş trendvagonu'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf158b08-2a97-4548-8983-d230bd6b1eb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['harika',\n",
       " 'bir',\n",
       " 'ürün',\n",
       " 'kesinlikle',\n",
       " 'tavsiye',\n",
       " 'ediyorum',\n",
       " 'kargo',\n",
       " 'çok',\n",
       " 'hızlıydı',\n",
       " 'ama',\n",
       " 'paket',\n",
       " 'biraz',\n",
       " 'ezilmişti',\n",
       " 'alışveriş',\n",
       " 'trendvagonu']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50675d7-5f45-46ed-8b5f-cb5c9b32ce1c",
   "metadata": {},
   "source": [
    "### Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a28b875d-b029-4393-a653-13b67c1ce4f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['harika',\n",
       " 'bir',\n",
       " 'ürün',\n",
       " 'kesinlikle',\n",
       " 'tavsiye',\n",
       " 'ediyorum',\n",
       " 'kargo',\n",
       " 'çok',\n",
       " 'hızlıydı',\n",
       " 'ama',\n",
       " 'paket',\n",
       " 'biraz',\n",
       " 'ezilmişti',\n",
       " 'alışveriş',\n",
       " 'trendvagonu',\n",
       " 'sanki',\n",
       " 'nereye',\n",
       " 'hepsi']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# örnek olması için bir kaç tane stop words ekleyelim:\n",
    "word_tokens.append(\"sanki\")\n",
    "word_tokens.append(\"nereye\")\n",
    "word_tokens.append(\"hepsi\") \n",
    "word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d166b3df-26b9-4a29-8df4-a33c9d2c893e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# nltk.download(\"stopwords\") # farklı dillerde en çok kullanılan stop words içeren veri seti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de91304f-5456-4d38-b79a-03733b5d557d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['acaba', 'ama', 'aslında', 'az', 'bazı', 'belki', 'biri', 'birkaç', 'birşey', 'biz', 'bu', 'çok', 'çünkü', 'da', 'daha', 'de', 'defa', 'diye', 'eğer', 'en', 'gibi', 'hem', 'hep', 'hepsi', 'her', 'hiç', 'için', 'ile', 'ise', 'kez', 'ki', 'kim', 'mı', 'mu', 'mü', 'nasıl', 'ne', 'neden', 'nerde', 'nerede', 'nereye', 'niçin', 'niye', 'o', 'sanki', 'şey', 'siz', 'şu', 'tüm', 've', 'veya', 'ya', 'yani']\n"
     ]
    }
   ],
   "source": [
    "stop_words = stopwords.words(\"turkish\")\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce7233e2-104a-4dce-8be0-379d78d4e30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Stop word' lerden temizlenmiş metin: ===\n",
      "['harika', 'bir', 'ürün', 'kesinlikle', 'tavsiye', 'ediyorum', 'kargo', 'hızlıydı', 'paket', 'biraz', 'ezilmişti', 'alışveriş', 'trendvagonu']\n",
      "\n",
      "=== Bulunan stop word' ler: ===\n",
      "['çok', 'ama', 'sanki', 'nereye', 'hepsi']\n"
     ]
    }
   ],
   "source": [
    "filtered_text = []\n",
    "stopwords_text = []\n",
    "for word in word_tokens:\n",
    "    if word not in stop_words:\n",
    "        filtered_text.append(word)\n",
    "    else:\n",
    "        stopwords_text.append(word)\n",
    "\n",
    "print(f\"=== Stop word' lerden temizlenmiş metin: ===\\n{filtered_text}\")\n",
    "print(f\"\\n=== Bulunan stop word' ler: ===\\n{stopwords_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3328920-068c-4ef4-a39f-cd418de21885",
   "metadata": {},
   "source": [
    "### Lemmatization vs Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "debb0f27-925e-4806-b073-923d72461e9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'two', 'mice', 'were', 'running', 'and', 'eating', 'better', 'cheese']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_text = \"The two mice were running and eating better cheese.\" # (İki fare koşuyor ve daha iyi peynir yiyordu.)\n",
    "new_text = new_text.replace(\".\",\"\")\n",
    "new_text = new_text.split()\n",
    "new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64a7a2dd-2ce1-450b-9019-f6da36226c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9575772e-e19c-4624-82b4-5ff3cf7544b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== stemming işleminden sonra: ===\n",
      "['the', 'two', 'mice', 'were', 'run', 'and', 'eat', 'better', 'chees']\n"
     ]
    }
   ],
   "source": [
    "stems = []\n",
    "for w in new_text:\n",
    "    stems.append(stemmer.stem(w))\n",
    "print(f\"=== stemming işleminden sonra: ===\\n{stems}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad0bced1-f981-4003-b2e3-ddda2e3b76db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatization\n",
    "# nltk.download('wordnet') # lemmatization işlemi için gerekli olan veri tabanı\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3fa1dc0-6592-42cb-a3cc-0ae1942d91a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== fiiler üzerinde lemmatization işleminden sonra: ===\n",
      "['The', 'two', 'mice', 'be', 'run', 'and', 'eat', 'better', 'cheese']\n",
      "\n",
      "=== isimler üzerinde lemmatization işleminden sonra: ===\n",
      "['The', 'two', 'mouse', 'were', 'running', 'and', 'eating', 'better', 'cheese']\n"
     ]
    }
   ],
   "source": [
    "lemmas_verb = [lemmatizer.lemmatize(w, pos=\"v\") for w in new_text]\n",
    "print(f\"=== fiiler üzerinde lemmatization işleminden sonra: ===\\n{lemmas_verb}\")\n",
    "\n",
    "lemmas_noun = [lemmatizer.lemmatize(w, pos=\"n\") for w in new_text]\n",
    "print(f\"\\n=== isimler üzerinde lemmatization işleminden sonra: ===\\n{lemmas_noun}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ec72f6-4da8-43a4-9101-45b2019e7162",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
