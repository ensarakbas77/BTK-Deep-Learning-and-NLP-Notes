{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6472cbf-eda9-45b0-814f-ff739aad40e7",
   "metadata": {},
   "source": [
    "### √ñdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7aa2007-270d-4a7a-adb9-796fc05f0539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TextProcessing notebook' u i√ßerisinde anlatƒ±lanlarƒ± tekrar etmek amacƒ±yla yaptƒ±ƒüƒ±m pratikler."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff36c3c6-b79c-467b-ba1a-c245109692aa",
   "metadata": {},
   "source": [
    "### Veri Temizleme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "838a6048-f576-4cc7-8cb3-0dd823c3d6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== K√º√ß√ºk harfe d√∂n√º≈üt√ºrme i≈üleminden sonraki metin: ===\n",
      "harika bir √ºr√ºn!!!  kesinlikle tavsiye ediyorum...     kargo √ßok hƒ±zlƒ±ydƒ± ama paket biraz ezilmi≈üti :(   #alƒ±≈üveri≈ü @trendvagonu\n",
      "\n",
      "=== noktalama i≈üaretlerini kaldƒ±rdƒ±ktan sonra olu≈üan metin: ===\n",
      "harika bir √ºr√ºn  kesinlikle tavsiye ediyorum     kargo √ßok hƒ±zlƒ±ydƒ± ama paket biraz ezilmi≈üti    alƒ±≈üveri≈ü trendvagonu\n"
     ]
    }
   ],
   "source": [
    "# b√ºy√ºk harf --> k√º√ß√ºk harf √ßevrimi\n",
    "text = \"Harika bir √ºr√ºn!!!  Kesinlikle tavsiye ediyorum...     Kargo √ßok hƒ±zlƒ±ydƒ± ama paket biraz ezilmi≈üti :(   #alƒ±≈üveri≈ü @trendvagonu\"\n",
    "text = text.lower()\n",
    "print(f\"=== K√º√ß√ºk harfe d√∂n√º≈üt√ºrme i≈üleminden sonraki metin: ===\\n{text}\")\n",
    "\n",
    "# noktalama i≈üaretlerini kaldƒ±rma\n",
    "import string\n",
    "\n",
    "text = text.translate(str.maketrans(\"\",\"\", string.punctuation))\n",
    "print(f\"\\n=== noktalama i≈üaretlerini kaldƒ±rdƒ±ktan sonra olu≈üan metin: ===\\n{text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00201ebb-3888-4972-813d-033d48449a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== bo≈üluklarƒ± kaldƒ±rdƒ±ktan sonra olu≈üna metin: ===\n",
      "harika bir √ºr√ºn kesinlikle tavsiye ediyorum kargo √ßok hƒ±zlƒ±ydƒ± ama paket biraz ezilmi≈üti alƒ±≈üveri≈ü trendvagonu\n"
     ]
    }
   ],
   "source": [
    "# bo≈üluklarƒ± temizleme\n",
    "text = \" \".join(text.split()) \n",
    "print(f\"=== bo≈üluklarƒ± kaldƒ±rdƒ±ktan sonra olu≈üna metin: ===\\n{text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6166877c-37ac-462f-ac64-43534b12d688",
   "metadata": {},
   "source": [
    "### Tokenizasyon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74173af8-2ae7-413b-abf8-598454efb113",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download(\"punkt\") # metni kelime ve c√ºmle bazƒ±nda tokenlara ayƒ±rabilmek i√ßin gerekli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c9c551a-b035-48f7-8110-8d1d14599ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== kelime bazlƒ± tokenizasyon: ===\n",
      "['harika', 'bir', '√ºr√ºn', 'kesinlikle', 'tavsiye', 'ediyorum', 'kargo', '√ßok', 'hƒ±zlƒ±ydƒ±', 'ama', 'paket', 'biraz', 'ezilmi≈üti', 'alƒ±≈üveri≈ü', 'trendvagonu']\n"
     ]
    }
   ],
   "source": [
    "word_tokens = nltk.word_tokenize(text)\n",
    "print(f\"=== kelime bazlƒ± tokenizasyon: ===\\n{word_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a53bd800-f1f9-4372-855e-216d3f3e7cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== c√ºmle bazlƒ± tokenizasyon: ===\n",
      "['Bu bir √∂devdir.', 'Veri √∂n i≈üleme pratiƒüi yapƒ±yorum.', '√ñƒüreniyorum...']\n"
     ]
    }
   ],
   "source": [
    "metin = \"Bu bir √∂devdir. Veri √∂n i≈üleme pratiƒüi yapƒ±yorum. √ñƒüreniyorum...\"\n",
    "sentence_tokens = nltk.sent_tokenize(metin)\n",
    "print(f\"=== c√ºmle bazlƒ± tokenizasyon: ===\\n{sentence_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cef87c1-07b9-40c0-b56e-982a9a5d386a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'harika bir √ºr√ºn kesinlikle tavsiye ediyorum kargo √ßok hƒ±zlƒ±ydƒ± ama paket biraz ezilmi≈üti alƒ±≈üveri≈ü trendvagonu'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf158b08-2a97-4548-8983-d230bd6b1eb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['harika',\n",
       " 'bir',\n",
       " '√ºr√ºn',\n",
       " 'kesinlikle',\n",
       " 'tavsiye',\n",
       " 'ediyorum',\n",
       " 'kargo',\n",
       " '√ßok',\n",
       " 'hƒ±zlƒ±ydƒ±',\n",
       " 'ama',\n",
       " 'paket',\n",
       " 'biraz',\n",
       " 'ezilmi≈üti',\n",
       " 'alƒ±≈üveri≈ü',\n",
       " 'trendvagonu']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50675d7-5f45-46ed-8b5f-cb5c9b32ce1c",
   "metadata": {},
   "source": [
    "### Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a28b875d-b029-4393-a653-13b67c1ce4f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['harika',\n",
       " 'bir',\n",
       " '√ºr√ºn',\n",
       " 'kesinlikle',\n",
       " 'tavsiye',\n",
       " 'ediyorum',\n",
       " 'kargo',\n",
       " '√ßok',\n",
       " 'hƒ±zlƒ±ydƒ±',\n",
       " 'ama',\n",
       " 'paket',\n",
       " 'biraz',\n",
       " 'ezilmi≈üti',\n",
       " 'alƒ±≈üveri≈ü',\n",
       " 'trendvagonu',\n",
       " 'sanki',\n",
       " 'nereye',\n",
       " 'hepsi']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# √∂rnek olmasƒ± i√ßin bir ka√ß tane stop words ekleyelim:\n",
    "word_tokens.append(\"sanki\")\n",
    "word_tokens.append(\"nereye\")\n",
    "word_tokens.append(\"hepsi\") \n",
    "word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d166b3df-26b9-4a29-8df4-a33c9d2c893e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# nltk.download(\"stopwords\") # farklƒ± dillerde en √ßok kullanƒ±lan stop words i√ßeren veri seti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de91304f-5456-4d38-b79a-03733b5d557d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['acaba', 'ama', 'aslƒ±nda', 'az', 'bazƒ±', 'belki', 'biri', 'birka√ß', 'bir≈üey', 'biz', 'bu', '√ßok', '√ß√ºnk√º', 'da', 'daha', 'de', 'defa', 'diye', 'eƒüer', 'en', 'gibi', 'hem', 'hep', 'hepsi', 'her', 'hi√ß', 'i√ßin', 'ile', 'ise', 'kez', 'ki', 'kim', 'mƒ±', 'mu', 'm√º', 'nasƒ±l', 'ne', 'neden', 'nerde', 'nerede', 'nereye', 'ni√ßin', 'niye', 'o', 'sanki', '≈üey', 'siz', '≈üu', 't√ºm', 've', 'veya', 'ya', 'yani']\n"
     ]
    }
   ],
   "source": [
    "stop_words = stopwords.words(\"turkish\")\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce7233e2-104a-4dce-8be0-379d78d4e30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Stop word' lerden temizlenmi≈ü metin: ===\n",
      "['harika', 'bir', '√ºr√ºn', 'kesinlikle', 'tavsiye', 'ediyorum', 'kargo', 'hƒ±zlƒ±ydƒ±', 'paket', 'biraz', 'ezilmi≈üti', 'alƒ±≈üveri≈ü', 'trendvagonu']\n",
      "\n",
      "=== Bulunan stop word' ler: ===\n",
      "['√ßok', 'ama', 'sanki', 'nereye', 'hepsi']\n"
     ]
    }
   ],
   "source": [
    "filtered_text = []\n",
    "stopwords_text = []\n",
    "for word in word_tokens:\n",
    "    if word not in stop_words:\n",
    "        filtered_text.append(word)\n",
    "    else:\n",
    "        stopwords_text.append(word)\n",
    "\n",
    "print(f\"=== Stop word' lerden temizlenmi≈ü metin: ===\\n{filtered_text}\")\n",
    "print(f\"\\n=== Bulunan stop word' ler: ===\\n{stopwords_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3328920-068c-4ef4-a39f-cd418de21885",
   "metadata": {},
   "source": [
    "### Lemmatization vs Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "debb0f27-925e-4806-b073-923d72461e9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'two', 'mice', 'were', 'running', 'and', 'eating', 'better', 'cheese']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_text = \"The two mice were running and eating better cheese.\" # (ƒ∞ki fare ko≈üuyor ve daha iyi peynir yiyordu.)\n",
    "new_text = new_text.replace(\".\",\"\")\n",
    "new_text = new_text.split()\n",
    "new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64a7a2dd-2ce1-450b-9019-f6da36226c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9575772e-e19c-4624-82b4-5ff3cf7544b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== stemming i≈üleminden sonra: ===\n",
      "['the', 'two', 'mice', 'were', 'run', 'and', 'eat', 'better', 'chees']\n"
     ]
    }
   ],
   "source": [
    "stems = []\n",
    "for w in new_text:\n",
    "    stems.append(stemmer.stem(w))\n",
    "print(f\"=== stemming i≈üleminden sonra: ===\\n{stems}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad0bced1-f981-4003-b2e3-ddda2e3b76db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatization\n",
    "# nltk.download('wordnet') # lemmatization i≈ülemi i√ßin gerekli olan veri tabanƒ±\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3fa1dc0-6592-42cb-a3cc-0ae1942d91a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== fiiler √ºzerinde lemmatization i≈üleminden sonra: ===\n",
      "['The', 'two', 'mice', 'be', 'run', 'and', 'eat', 'better', 'cheese']\n",
      "\n",
      "=== isimler √ºzerinde lemmatization i≈üleminden sonra: ===\n",
      "['The', 'two', 'mouse', 'were', 'running', 'and', 'eating', 'better', 'cheese']\n"
     ]
    }
   ],
   "source": [
    "lemmas_verb = [lemmatizer.lemmatize(w, pos=\"v\") for w in new_text]\n",
    "print(f\"=== fiiler √ºzerinde lemmatization i≈üleminden sonra: ===\\n{lemmas_verb}\")\n",
    "\n",
    "lemmas_noun = [lemmatizer.lemmatize(w, pos=\"n\") for w in new_text]\n",
    "print(f\"\\n=== isimler √ºzerinde lemmatization i≈üleminden sonra: ===\\n{lemmas_noun}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f11cb74-8eb6-4e46-b893-0ab561013e12",
   "metadata": {},
   "source": [
    "## DataFrame ile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8342c683-271d-401b-94f9-43116c6fd852",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "869bd8a6-6f63-4945-93dc-f34fc1db0d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = {\n",
    "    \"1\": \"Wow!!! This   product is AMAZING... <br> Check it out at https://example.com/deal! 5 stars from me.\",\n",
    "    \"2\": \"Please contact support@tech-corp.com regarding order #998877. The total cost was $199.99 (USD). DON'T forget!\",\n",
    "    \"3\": \"Just watched the NEW movie! üé• It's kinda long (3 hrs) but totally worth it. #movies #fun @cinema_lover\",\n",
    "    \"4\": \"<b>Warning:</b> The system crashed at 12:00 PM. Error code: [404]. Please restart the server... ASAP!!!\",\n",
    "    \"5\": \"I've been running for 2 hours; it's exhausting. I shouldn't have eaten that heavy lunch...   Maybe tomorrow's training will be better.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3b11995e-559f-48eb-bcd5-c87a65101f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "values = texts.values()\n",
    "col_names = texts.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d60b1bb-821c-441a-a65a-4a7dea0729dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wow!!! This   product is AMAZING... &lt;br&gt; Check it out at https://example.com/deal! 5 stars from me.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Please contact support@tech-corp.com regarding order #998877. The total cost was $199.99 (USD). DON'T forget!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Just watched the NEW movie! üé• It's kinda long (3 hrs) but totally worth it. #movies #fun @cinema_lover</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;b&gt;Warning:&lt;/b&gt; The system crashed at 12:00 PM. Error code: [404]. Please restart the server... ASAP!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I've been running for 2 hours; it's exhausting. I shouldn't have eaten that heavy lunch...   Maybe tomorrow's training will be better.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                     text\n",
       "1                                     Wow!!! This   product is AMAZING... <br> Check it out at https://example.com/deal! 5 stars from me.\n",
       "2                           Please contact support@tech-corp.com regarding order #998877. The total cost was $199.99 (USD). DON'T forget!\n",
       "3                                  Just watched the NEW movie! üé• It's kinda long (3 hrs) but totally worth it. #movies #fun @cinema_lover\n",
       "4                                 <b>Warning:</b> The system crashed at 12:00 PM. Error code: [404]. Please restart the server... ASAP!!!\n",
       "5  I've been running for 2 hours; it's exhausting. I shouldn't have eaten that heavy lunch...   Maybe tomorrow's training will be better."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict(texts, orient='index', columns=['text'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1276361e-32ba-40bb-9b38-897cfff76616",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text    Wow!!! This   product is AMAZING... <br> Check it out at https://example.com/deal! 5 stars from me.\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb3fe50-90bb-4aba-a844-824e37b3d606",
   "metadata": {},
   "source": [
    "### üõ†Ô∏è Metin √ñn ƒ∞≈üleme Boru Hattƒ± (Text Preprocessing Pipeline)\n",
    "\n",
    "Doƒüal Dil ƒ∞≈üleme (NLP) modellerinin ba≈üarƒ±sƒ±, verinin kalitesine ve temizliƒüine doƒürudan baƒülƒ±dƒ±r. Ham metin verileri genellikle g√ºr√ºlt√ºl√ºd√ºr. Bu √ßalƒ±≈ümada, metinleri makine √∂ƒürenmesi algoritmalarƒ± i√ßin standart hale getirmek amacƒ±yla a≈üaƒüƒ±daki \"Pipeline\" (i≈ülem sƒ±rasƒ±) uygulanmƒ±≈ütƒ±r:\n",
    "\n",
    "1.  **Kƒ±saltmalarƒ±n A√ßƒ±lmasƒ± (Contraction Expansion):** Metindeki anlam b√ºt√ºnl√ºƒü√ºn√º korumak ve √∂zellikle olumsuzluk eklerini (*not*) kaybetmemek i√ßin kƒ±saltmalar a√ßƒ±ldƒ± (√ñrn: *won't* $\\rightarrow$ *will not*).\n",
    "2.  **Normalizasyon (Case Folding):** Kelime daƒüarcƒ±ƒüƒ±nƒ± standartla≈ütƒ±rmak i√ßin t√ºm harfler k√º√ß√ºk harfe (lowercase) √ßevrildi.\n",
    "3.  **G√ºr√ºlt√º Temizliƒüi (Noise Removal):**\n",
    "    * **HTML & URL:** Metin i√ßeriƒüiyle ilgisi olmayan web etiketleri ve linkler temizlendi.\n",
    "    * **√ñzel Karakterler:** Harf ve rakam dƒ±≈üƒ±ndaki karakterler (noktalama i≈üaretleri, emojiler vb.) bo≈üluk ile deƒüi≈ütirildi.\n",
    "    * **Sayƒ±lar:** Analiz baƒülamƒ±nda g√ºr√ºlt√º olu≈üturabileceƒüi i√ßin sayƒ±sal veriler kaldƒ±rƒ±ldƒ±.\n",
    "4.  **Tokenizasyon (Tokenization):** C√ºmle b√ºt√ºnl√ºƒü√º bozularak metin kelime par√ßalarƒ±na (token) ayrƒ±ldƒ±.\n",
    "5.  **Stopwords Temizliƒüi:** Metnin ana fikrini etkilemeyen sƒ±k kullanƒ±lan baƒüla√ßlar (the, is, and vb.) filtrelendi. *Not: Olumsuzluk bildiren 'not', 'no' gibi kelimeler korunmu≈ütur.*\n",
    "6.  **Lemmatization (K√∂k Bulma):** Kelimelerin √ßekim ekleri atƒ±larak s√∂zl√ºk k√∂klerine indirgendi (√ñrn: *running* $\\rightarrow$ *run*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bdfbeba3-8931-49eb-b0b3-a77ad9f70017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text              I've been running for 2 hours; it's exhausting. I shouldn't have eaten that heavy lunch...   Maybe tomorrow's training will be better.\n",
      "clean_text    I have been running for 2 hours; it is exhausting. I should not have eaten that heavy lunch...   Maybe tomorrow's training will be better.\n",
      "Name: 5, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import contractions\n",
    "\n",
    "# ADIM 1: Kƒ±saltmalarƒ± Geni≈ületme = I've --> I have\n",
    "# lambda x: contractions.fix(x) fonksiyonunu her satƒ±ra uyguluyoruz.\n",
    "df[\"clean_text\"] = df[\"text\"].apply(lambda x: contractions.fix(x))\n",
    "print(df.iloc[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8aab84a0-637d-41b9-9a60-5a23af7a9664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K√º√ß√ºk Harfe √áevirme:\n",
      " text          Wow!!! This   product is AMAZING... <br> Check it out at https://example.com/deal! 5 stars from me.\n",
      "clean_text    wow!!! this   product is amazing... <br> check it out at https://example.com/deal! 5 stars from me.\n",
      "Name: 1, dtype: object\n",
      "\n",
      "HTML & URL Kaldƒ±rma:\n",
      " wow!!! this   product is amazing...  check it out at  5 stars from me.\n",
      "\n",
      "Noktalama & √ñzel ƒ∞≈üaretleri Kaldƒ±rma:\n",
      " wow    this   product is amazing     check it out at  5 stars from me \n",
      "\n",
      "Sayƒ±larƒ± Kaldƒ±rma:\n",
      " wow    this   product is amazing     check it out at   stars from me \n",
      "\n",
      "Bo≈üluklarƒ± Kaldƒ±rma:\n",
      " wow this product is amazing check it out at stars from me\n"
     ]
    }
   ],
   "source": [
    "# ADIM 2: Diƒüer ƒ∞≈ülemler\n",
    "\n",
    "# K√º√ß√ºk Harfe √áevirme\n",
    "df[\"clean_text\"] = df[\"clean_text\"].str.lower()\n",
    "print(\"K√º√ß√ºk Harfe √áevirme:\\n\",df.iloc[0])\n",
    "print()\n",
    "\n",
    "# 2. Adƒ±m: HTML ve URL Temizliƒüi\n",
    "df[\"clean_text\"] = df[\"clean_text\"].str.replace(r'http\\S+|www\\.\\S+', '', regex=True)\n",
    "df[\"clean_text\"] = df[\"clean_text\"].str.replace(r'<.*?>', '', regex=True)\n",
    "print(\"HTML & URL Kaldƒ±rma:\\n\",df[\"clean_text\"].iloc[0])\n",
    "print()\n",
    "\n",
    "# 3. Adƒ±m: Noktalama ƒ∞≈üaretleri ve √ñzel Karakterler (Emoji dahil)\n",
    "# df[\"clean_text\"] = df[\"clean_text\"].str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "df[\"clean_text\"] = df[\"clean_text\"].str.replace(r'[^a-zA-Z0-9\\s]', ' ', regex=True)\n",
    "print(\"Noktalama & √ñzel ƒ∞≈üaretleri Kaldƒ±rma:\\n\",df[\"clean_text\"].iloc[0])\n",
    "print()\n",
    "\n",
    "# 4. Adƒ±m: Sayƒ±larƒ± Kaldƒ±rma\n",
    "df[\"clean_text\"] = df[\"clean_text\"].str.replace(r'\\d+', '', regex=True)\n",
    "print(\"Sayƒ±larƒ± Kaldƒ±rma:\\n\",df[\"clean_text\"].iloc[0])\n",
    "print()\n",
    "\n",
    "# 5. Adƒ±m: Fazla Bo≈üluklarƒ± Kaldƒ±rma\n",
    "df[\"clean_text\"] = df[\"clean_text\"].str.split().str.join(\" \")\n",
    "print(\"Bo≈üluklarƒ± Kaldƒ±rma:\\n\",df[\"clean_text\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fa3a7f0b-e495-40e4-82e6-a658b3f58a76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text          I've been running for 2 hours; it's exhausting. I shouldn't have eaten that heavy lunch...   Maybe tomorrow's training will be better.\n",
       "clean_text          i have been running for hours it is exhausting i should not have eaten that heavy lunch maybe tomorrow s training will be better\n",
       "Name: 5, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8859b8b8-4079-458e-a826-455a446df068",
   "metadata": {},
   "source": [
    "### Tokenizasyon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e0d1c00a-f757-4b39-8427-e0caff948922",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = []\n",
    "\n",
    "# Her satƒ±rƒ± geziyoruz\n",
    "for text in df[\"clean_text\"]:\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokenized_data.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9b32421c-e63f-4b62-9829-12e9d760e235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize edilmi≈ü hali (Liste):\n",
      "['i', 'have', 'been', 'running', 'for', 'hours', 'it', 'is', 'exhausting', 'i', 'should', 'not', 'have', 'eaten', 'that', 'heavy', 'lunch', 'maybe', 'tomorrow', 's', 'training', 'will', 'be', 'better']\n"
     ]
    }
   ],
   "source": [
    "df[\"tokens\"] = tokenized_data\n",
    "\n",
    "print(\"Tokenize edilmi≈ü hali (Liste):\")\n",
    "print(df[\"tokens\"].iloc[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81ce233-641e-46a3-a8d3-ad2595f76516",
   "metadata": {},
   "source": [
    "### StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "89b52e28-a315-44f9-8fae-e7f9c755ce4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'shouldn', 'be', 'when', 'no', 'against', 'yours', 'own', \"they'll\", 'between', 'once', 'during', \"i'd\", 'it', \"we'd\", 'all', 'am', \"he'd\", \"she'll\", \"i'll\", 'aren', 'in', 'an', 'above', 'further', 'isn', \"that'll\", 'this', 'our', 'out', \"haven't\", 'some', 'what', 'these', 'same', 'y', 'only', 'because', 'ma', \"shan't\", 'haven', 'their', \"she's\", 'they', 'that', \"hadn't\", 'did', 'down', 'wasn', 'not', 'she', 'does', 'here', 'so', 't', 'as', 'shan', 'ain', 'but', 'herself', 've', 'was', \"we'll\", 'there', \"he'll\", 'the', \"we've\", \"it'd\", 'being', 'himself', 'any', 'after', 'hers', 'his', \"it'll\", \"aren't\", \"you'd\", 'very', 'needn', 'now', 'too', 'theirs', 'those', \"you've\", 'yourself', 'had', 'more', 'm', 'do', \"couldn't\", \"they're\", 'doing', 'with', \"mustn't\", 'up', 'how', \"i'm\", 'is', 'why', 'from', 'then', 'on', 'can', 'few', 'for', 'having', 'its', 'mustn', 'to', 'were', 'won', 'your', \"hasn't\", \"he's\", 'most', \"it's\", 'her', 'doesn', 'below', \"should've\", \"doesn't\", 'into', 'ours', 'will', \"isn't\", 'than', 'about', 'other', 'wouldn', 's', \"shouldn't\", 'each', \"they've\", 'where', 'a', 'or', 'under', 'off', 'before', 'at', \"didn't\", 'if', 'just', 'll', \"mightn't\", 'couldn', 'through', 'i', \"wasn't\", \"won't\", 'of', 'whom', \"needn't\", 'and', 'again', \"i've\", 'itself', 'over', 'both', 'have', 'mightn', 'didn', 'until', 'nor', 'has', 'myself', 'such', \"you'll\", 'should', 'them', 'me', 'he', 'd', \"we're\", 'been', 'we', \"they'd\", 'him', \"she'd\", 'hasn', 'ourselves', 'my', \"don't\", \"wouldn't\", 'by', 'weren', 'you', 'yourselves', 'don', 'hadn', 'while', \"you're\", 're', 'o', 'who', \"weren't\", 'themselves', 'are', 'which'}\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac65d0b5-da6c-4f6c-9e06-06293156472a",
   "metadata": {},
   "source": [
    "### üìù Neden √ñnce Kƒ±saltmalarƒ± A√ßƒ±yoruz? (Contraction Expansion vs. Stopwords)\n",
    "Stopwords listelerinde genellikle \"I've\", \"won't\" gibi ifadeler bulunabilir veya tokenizer bunlarƒ± hatalƒ± b√∂lebilir. Ancak bu adƒ±mƒ± Stopwords'ten √∂nce yapmamƒ±zƒ±n temel sebebi olumsuzluk (negation) bilgisini korumaktƒ±r.\n",
    "\n",
    "√ñrnek: I've $\\rightarrow$ I have olarak a√ßƒ±ldƒ±ƒüƒ±nda, her iki kelime de stopword olduƒüu i√ßin silinebilir. Bu bir sorun deƒüildir.\n",
    "\n",
    "Kritik Nokta: won't $\\rightarrow$ will not olarak a√ßƒ±ldƒ±ƒüƒ±nda; will silinse bile not kelimesi metinde kalƒ±r (Stopwords listesinden 'not' √ßƒ±karƒ±ldƒ±ƒüƒ± varsayƒ±larak). Bu sayede c√ºmlenin olumsuz anlamƒ± korunmu≈ü olur.\n",
    "\n",
    "| Durum | Orijinal C√ºmle | ƒ∞≈ülem Sonucu | Modelin Anladƒ±ƒüƒ± | Hata Durumu |\n",
    "| :--- | :--- | :--- | :--- | :--- |\n",
    "| **Kƒ±saltma A√ßƒ±lmazsa** | *\"I won't go.\"* | *\"go\"* | Gitmek (Olumlu/N√∂tr) | ‚ùå Anlam Deƒüi≈üti |\n",
    "| **Kƒ±saltma A√ßƒ±lƒ±rsa** | *\"I will not go.\"* | *\"not go\"* | Gitmemek (Olumsuz) | ‚úÖ Doƒüru |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "439a6d44-db5e-4503-ab63-032ed684a244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:\n",
      " <b>Warning:</b> The system crashed at 12:00 PM. Error code: [404]. Please restart the server... ASAP!!!\n",
      "\n",
      "Clean Text:\n",
      " warning the system crashed at pm error code please restart the server as soon as possible\n",
      "\n",
      "Tokenizasyon Sonrasƒ±:\n",
      " ['warning', 'the', 'system', 'crashed', 'at', 'pm', 'error', 'code', 'please', 'restart', 'the', 'server', 'as', 'soon', 'as', 'possible']\n",
      "\n",
      "Stop Word Sonrasƒ±:\n",
      " ['warning', 'system', 'crashed', 'pm', 'error', 'code', 'please', 'restart', 'server', 'soon', 'possible']\n"
     ]
    }
   ],
   "source": [
    "filtered_data = []\n",
    "\n",
    "for tokens in df[\"tokens\"]:\n",
    "    clean_tokens = [word for word in tokens if word not in stop_words]\n",
    "    filtered_data.append(clean_tokens)\n",
    "\n",
    "df[\"no_stopwords\"] = filtered_data\n",
    "\n",
    "print(\"Text:\\n\",df[\"text\"].iloc[3])\n",
    "print(\"\\nClean Text:\\n\",df[\"clean_text\"].iloc[3])\n",
    "print(\"\\nTokenizasyon Sonrasƒ±:\\n\",df[\"tokens\"].iloc[3])\n",
    "print(\"\\nStop Word Sonrasƒ±:\\n\",df[\"no_stopwords\"].iloc[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d390ab5c-aefc-4f1c-bcf3-278a6af92958",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b16dc4a2-f69b-48a1-9e43-e3e242959fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:\n",
      " Just watched the NEW movie! üé• It's kinda long (3 hrs) but totally worth it. #movies #fun @cinema_lover\n",
      "\n",
      "Clean Text:\n",
      " just watched the new movie it is kind of long hrs but totally worth it movies fun cinema lover\n",
      "\n",
      "Tokenizasyon Sonrasƒ±:\n",
      " ['just', 'watched', 'the', 'new', 'movie', 'it', 'is', 'kind', 'of', 'long', 'hrs', 'but', 'totally', 'worth', 'it', 'movies', 'fun', 'cinema', 'lover']\n",
      "\n",
      "Stop Word Sonrasƒ±:\n",
      " ['watched', 'new', 'movie', 'kind', 'long', 'hrs', 'totally', 'worth', 'movies', 'fun', 'cinema', 'lover']\n",
      "\n",
      "Lemmatization Sonrasƒ±:\n",
      " ['watch', 'new', 'movie', 'kind', 'long', 'hrs', 'totally', 'worth', 'movies', 'fun', 'cinema', 'lover']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemma_data = []\n",
    "\n",
    "for tokens in df[\"no_stopwords\"]:\n",
    "    # Her kelimenin k√∂k√ºn√º buluyoruz\n",
    "    # pos='v' parametresi fiil (verb) olarak deƒüerlendirmesini saƒülar (running -> run)\n",
    "    # Genel kullanƒ±m i√ßin pos parametresi vermeden de kullanabilirsiniz: lemmatizer.lemmatize(word)\n",
    "    root_tokens = [lemmatizer.lemmatize(word, pos='v') for word in tokens] \n",
    "    lemma_data.append(root_tokens)\n",
    "\n",
    "df[\"lemmas\"] = lemma_data\n",
    "\n",
    "print(\"Text:\\n\",df[\"text\"].iloc[2])\n",
    "print(\"\\nClean Text:\\n\",df[\"clean_text\"].iloc[2])\n",
    "print(\"\\nTokenizasyon Sonrasƒ±:\\n\",df[\"tokens\"].iloc[2])\n",
    "print(\"\\nStop Word Sonrasƒ±:\\n\",df[\"no_stopwords\"].iloc[2])\n",
    "print(\"\\nLemmatization Sonrasƒ±:\\n\",df[\"lemmas\"].iloc[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2a90c26a-a693-4363-a358-7c05e765f090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sonu√ß:\n",
      "run hours exhaust eat heavy lunch maybe tomorrow train better\n"
     ]
    }
   ],
   "source": [
    "final_sentences = []\n",
    "\n",
    "for tokens in df[\"lemmas\"]:\n",
    "    sentence = \" \".join(tokens)\n",
    "    final_sentences.append(sentence)\n",
    "\n",
    "df[\"final_text\"] = final_sentences\n",
    "\n",
    "print(\"Sonu√ß:\")\n",
    "print(df[\"final_text\"].iloc[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "18701a86-b055-49be-83f0-e34029889f3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>no_stopwords</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>final_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wow!!! This   product is AMAZING... &lt;br&gt; Check it out at https://example.com/deal! 5 stars from me.</td>\n",
       "      <td>wow this product is amazing check it out at stars from me</td>\n",
       "      <td>[wow, this, product, is, amazing, check, it, out, at, stars, from, me]</td>\n",
       "      <td>[wow, product, amazing, check, stars]</td>\n",
       "      <td>[wow, product, amaze, check, star]</td>\n",
       "      <td>wow product amaze check star</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Please contact support@tech-corp.com regarding order #998877. The total cost was $199.99 (USD). DON'T forget!</td>\n",
       "      <td>please contact support tech corp com regarding order the total cost was usd do not forget</td>\n",
       "      <td>[please, contact, support, tech, corp, com, regarding, order, the, total, cost, was, usd, do, not, forget]</td>\n",
       "      <td>[please, contact, support, tech, corp, com, regarding, order, total, cost, usd, forget]</td>\n",
       "      <td>[please, contact, support, tech, corp, com, regard, order, total, cost, usd, forget]</td>\n",
       "      <td>please contact support tech corp com regard order total cost usd forget</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                            text  \\\n",
       "1            Wow!!! This   product is AMAZING... <br> Check it out at https://example.com/deal! 5 stars from me.   \n",
       "2  Please contact support@tech-corp.com regarding order #998877. The total cost was $199.99 (USD). DON'T forget!   \n",
       "\n",
       "                                                                                  clean_text  \\\n",
       "1                                  wow this product is amazing check it out at stars from me   \n",
       "2  please contact support tech corp com regarding order the total cost was usd do not forget   \n",
       "\n",
       "                                                                                                       tokens  \\\n",
       "1                                      [wow, this, product, is, amazing, check, it, out, at, stars, from, me]   \n",
       "2  [please, contact, support, tech, corp, com, regarding, order, the, total, cost, was, usd, do, not, forget]   \n",
       "\n",
       "                                                                              no_stopwords  \\\n",
       "1                                                    [wow, product, amazing, check, stars]   \n",
       "2  [please, contact, support, tech, corp, com, regarding, order, total, cost, usd, forget]   \n",
       "\n",
       "                                                                                 lemmas  \\\n",
       "1                                                    [wow, product, amaze, check, star]   \n",
       "2  [please, contact, support, tech, corp, com, regard, order, total, cost, usd, forget]   \n",
       "\n",
       "                                                                final_text  \n",
       "1                                             wow product amaze check star  \n",
       "2  please contact support tech corp com regard order total cost usd forget  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcef8e62-e5a4-48f8-b388-d26a8e046a97",
   "metadata": {},
   "source": [
    "### Profesyonel Yakla≈üƒ±m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6c998527-bec3-4933-94cc-10c5d47b3db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import contractions\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4f7a243e-5616-4c69-8e0e-aaa59c03d985",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wow!!! This   product is AMAZING... &lt;br&gt; Check it out at https://example.com/deal! 5 stars from me.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Please contact support@tech-corp.com regarding order #998877. The total cost was $199.99 (USD). DON'T forget!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Just watched the NEW movie! üé• It's kinda long (3 hrs) but totally worth it. #movies #fun @cinema_lover</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;b&gt;Warning:&lt;/b&gt; The system crashed at 12:00 PM. Error code: [404]. Please restart the server... ASAP!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I've been running for 2 hours; it's exhausting. I shouldn't have eaten that heavy lunch...   Maybe tomorrow's training will be better.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                     text\n",
       "1                                     Wow!!! This   product is AMAZING... <br> Check it out at https://example.com/deal! 5 stars from me.\n",
       "2                           Please contact support@tech-corp.com regarding order #998877. The total cost was $199.99 (USD). DON'T forget!\n",
       "3                                  Just watched the NEW movie! üé• It's kinda long (3 hrs) but totally worth it. #movies #fun @cinema_lover\n",
       "4                                 <b>Warning:</b> The system crashed at 12:00 PM. Error code: [404]. Please restart the server... ASAP!!!\n",
       "5  I've been running for 2 hours; it's exhausting. I shouldn't have eaten that heavy lunch...   Maybe tomorrow's training will be better."
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict(texts, orient='index', columns=['text'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8a3af735-2f04-4da6-adaf-2a6a47f947b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_pipeline(text):\n",
    "    \"\"\"\n",
    "    Bu fonksiyon ham metni alƒ±r ve t√ºm NLP √∂n i≈üleme adƒ±mlarƒ±ndan ge√ßirerek\n",
    "    temizlenmi≈ü string olarak geri d√∂nd√ºr√ºr.\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return \"\"\n",
    "    \n",
    "    # 1. Kƒ±saltmalarƒ± Geni≈ületme (I've -> I have)\n",
    "    text = contractions.fix(text)\n",
    "    \n",
    "    # 2. K√º√ß√ºk Harfe √áevirme\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 3. HTML ve URL Temizliƒüi (Regex ile)\n",
    "    # URL'leri sil\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
    "    # HTML etiketlerini sil\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    # 4. √ñzel Karakter Temizliƒüi (Harf ve Sayƒ± dƒ±≈üƒ±ndakileri BO≈ûLUK yap)\n",
    "    # [^a-z0-9\\s] -> a-z, 0-9 ve bo≈üluk OLMAYAN her ≈üey\n",
    "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "    \n",
    "    # 5. Sayƒ±larƒ± Kaldƒ±rma (ƒ∞steƒüe baƒülƒ±, projenize g√∂re kapatabilirsiniz)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # 6. Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # 7. Stopwords & Lemmatization (List Comprehension)\n",
    "    # Hem stop word'leri atƒ±yoruz hem de kalanlarƒ± k√∂k√ºne indiriyoruz.\n",
    "    clean_tokens = [\n",
    "        lemmatizer.lemmatize(word, pos='v') \n",
    "        for word in tokens \n",
    "        if word not in stop_words\n",
    "    ]\n",
    "    \n",
    "    # 8. Tekrar Birle≈ütirme (List -> String)\n",
    "    return \" \".join(clean_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "40eb1462-72aa-4b6c-a7cb-fb1fab227aaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wow!!! This   product is AMAZING... &lt;br&gt; Check it out at https://example.com/deal! 5 stars from me.</td>\n",
       "      <td>wow product amaze check star</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Please contact support@tech-corp.com regarding order #998877. The total cost was $199.99 (USD). DON'T forget!</td>\n",
       "      <td>please contact support tech corp com regard order total cost usd forget</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Just watched the NEW movie! üé• It's kinda long (3 hrs) but totally worth it. #movies #fun @cinema_lover</td>\n",
       "      <td>watch new movie kind long hrs totally worth movies fun cinema lover</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;b&gt;Warning:&lt;/b&gt; The system crashed at 12:00 PM. Error code: [404]. Please restart the server... ASAP!!!</td>\n",
       "      <td>warn system crash pm error code please restart server soon possible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I've been running for 2 hours; it's exhausting. I shouldn't have eaten that heavy lunch...   Maybe tomorrow's training will be better.</td>\n",
       "      <td>run hours exhaust eat heavy lunch maybe tomorrow train better</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                     text  \\\n",
       "1                                     Wow!!! This   product is AMAZING... <br> Check it out at https://example.com/deal! 5 stars from me.   \n",
       "2                           Please contact support@tech-corp.com regarding order #998877. The total cost was $199.99 (USD). DON'T forget!   \n",
       "3                                  Just watched the NEW movie! üé• It's kinda long (3 hrs) but totally worth it. #movies #fun @cinema_lover   \n",
       "4                                 <b>Warning:</b> The system crashed at 12:00 PM. Error code: [404]. Please restart the server... ASAP!!!   \n",
       "5  I've been running for 2 hours; it's exhausting. I shouldn't have eaten that heavy lunch...   Maybe tomorrow's training will be better.   \n",
       "\n",
       "                                                                clean_text  \n",
       "1                                             wow product amaze check star  \n",
       "2  please contact support tech corp com regard order total cost usd forget  \n",
       "3      watch new movie kind long hrs totally worth movies fun cinema lover  \n",
       "4      warn system crash pm error code please restart server soon possible  \n",
       "5            run hours exhaust eat heavy lunch maybe tomorrow train better  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"clean_text\"] = df[\"text\"].apply(clean_text_pipeline)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5de519c-3c8b-4ab1-94f6-bb69af7cef71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
